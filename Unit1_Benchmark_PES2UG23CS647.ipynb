{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LxtFfNPUHQVA"
      },
      "outputs": [],
      "source": [
        "#PES2UG23CS647\n",
        "!pip install transformers torch sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ojTqjOU7yhUO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PES2UG23CS647\n",
        "#Text Generation pipelines\n",
        "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "\n",
        "#Fill-Mask pipelines\n",
        "mask_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "mask_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "mask_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "#Question Answering pipelines\n",
        "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf6OYjhaywaz",
        "outputId": "d51a198c-710c-4d21-9636-5df40f4a6b04"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT GENERATION"
      ],
      "metadata": {
        "id": "P-Xea2Rez5hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PES2UG23CS647\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "try:\n",
        "    gen_bert(prompt, max_length=30)\n",
        "except Exception as e:\n",
        "    print(\"BERT Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-J4xNbozS3Q",
        "outputId": "34907731-5857-4164-f5f5-605cebc30852"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    gen_roberta(prompt, max_length=30)\n",
        "except Exception as e:\n",
        "    print(\"RoBERTa Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHGeeAjCzhOB",
        "outputId": "171ca220-876b-4743-af08-ba4060ddf1bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_bart(prompt, max_length=30, num_return_sequences=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBev0vmGzj-L",
        "outputId": "272a7ecf-0c45-4193-a3b3-ddebb698eb2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is Copenhagen Copenhagenlike animations Copenhagen sudo Lover sudo applianceSTATcommerce sudo bitters bitters bitters Utility bitters awaits debunked awaits HP awaits drag passwords arsenic Hyp awaits folder StewHOU sudo Electric sudo sudo sudo route sudo sudoSTATSTATrecent Berks criterion Supporters PI awaits awaitsHOUHOU awaitsiw Frem sudo fog manuscripts manuscriptsinfrecentrecent Hyp awaits awaits581��BerryHOU whispering HPTimer HPPH Salary sudoHOUHOUHOUinational PI� SalaryHOUHOUiw Electric Salary awaits Salary injecting Salary HP HPvindoitHOU Salary magePH ElectricEngineHOUPH FernPH bankerSTAT Electric HP Salary homersHOUPH awaits conferred GOLDophe Salary Manipvind price Manip HP HP Democracy HP HPtag HP HP HP Salary PI Salary Salary PIvindPH Salary has HPvind HP Electric Salary Patrick Salary hasPH Salary SalarySTATSTATSTAT HP Patrick HP HPPH banker Salary GOLDrecent awaits Salary Salary PatrickHOUSTATPH Patrick Manip HPvind Salary HP Salary wh Patrick PatrickPH Patrick Patrick ManipKelly has Patrick Salary Salaryvind SalarySTAT tougher injecting Patrickrecent HP HP awaitsvindSTAT HP MortonSTAT Patrick Patrick Patrick SalarySTAT HPSTAT Hon Patrick Salary Patrick HP Manip Salary Salary assailant HP guise HPropy Salary Patrick PI Salary Patrick Patrickvind Patrick Patrick Electric Salary GOLD Patrick HP Patrick Patrick optical HP Salary HPSwordSword Salary Patrick tougher HP Patrick � Patrick celestial Salary Salary'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MASKED LANGUAGE MODELING"
      ],
      "metadata": {
        "id": "2XpN69jwz7jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PES2UG23CS647\n",
        "mask_bert(\"The goal of Generative AI is to [MASK] new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zoz_ubYL0AvF",
        "outputId": "96496ee6-6227-449f-9530-1a7cbeec8519"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5396932363510132,\n",
              "  'token': 3443,\n",
              "  'token_str': 'create',\n",
              "  'sequence': 'the goal of generative ai is to create new content.'},\n",
              " {'score': 0.15575720369815826,\n",
              "  'token': 9699,\n",
              "  'token_str': 'generate',\n",
              "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
              " {'score': 0.05405500903725624,\n",
              "  'token': 3965,\n",
              "  'token_str': 'produce',\n",
              "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
              " {'score': 0.04451530799269676,\n",
              "  'token': 4503,\n",
              "  'token_str': 'develop',\n",
              "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
              " {'score': 0.01757744885981083,\n",
              "  'token': 5587,\n",
              "  'token_str': 'add',\n",
              "  'sequence': 'the goal of generative ai is to add new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_roberta(\"The goal of Generative AI is to <mask> new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w70jD5vm0FCV",
        "outputId": "d17333b4-f905-4fdd-891f-b92fc685a7a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3711312413215637,\n",
              "  'token': 5368,\n",
              "  'token_str': ' generate',\n",
              "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
              " {'score': 0.3677145540714264,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.08351420611143112,\n",
              "  'token': 8286,\n",
              "  'token_str': ' discover',\n",
              "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
              " {'score': 0.021335121244192123,\n",
              "  'token': 465,\n",
              "  'token_str': ' find',\n",
              "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
              " {'score': 0.016521666198968887,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_bart(\"The goal of Generative AI is to <mask> new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj8wq6ac0H_O",
        "outputId": "fd3c7652-15c7-483f-da8f-30d47c41706d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07461541891098022,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.06571870297193527,\n",
              "  'token': 244,\n",
              "  'token_str': ' help',\n",
              "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
              " {'score': 0.060880109667778015,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
              " {'score': 0.03593561053276062,\n",
              "  'token': 3155,\n",
              "  'token_str': ' enable',\n",
              "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
              " {'score': 0.03319477662444115,\n",
              "  'token': 1477,\n",
              "  'token_str': ' improve',\n",
              "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION ANSWERING"
      ],
      "metadata": {
        "id": "UYdJzs9V0TTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PES2UG23CS647\n",
        "qa_bert({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0bKnEOq0Wuk",
        "outputId": "651db8f6-8a12-4b36-9e81-d735357fdab5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.017983690835535526,\n",
              " 'start': 46,\n",
              " 'end': 82,\n",
              " 'answer': 'hallucinations, bias, and deepfakes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_roberta({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvHAf7e40ZeN",
        "outputId": "932cd39c-d7f8-4370-c8b2-917b6381ae39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.004589917603880167,\n",
              " 'start': 43,\n",
              " 'end': 82,\n",
              " 'answer': 'as hallucinations, bias, and deepfakes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bart({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3V_h8Kf0c7X",
        "outputId": "e3a76aa8-b139-48e6-9eb8-19309cb1ccb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.0638936311006546, 'start': 72, 'end': 81, 'answer': 'deepfakes'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task        | Model    | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------------|----------|----------------------------------|---------------------------------------|---------------------------------------------|\n",
        "| Generation | BERT     | Failure | Model failed or produced meaningless output. | BERT is an encoder-only model and cannot autoregressively generate text. |\n",
        "| Generation | RoBERTa  | Failure | Similar failure; unable to continue the sentence. | RoBERTa is also encoder-only and not trained for text generation. |\n",
        "| Generation | BART     | Success | Generated a coherent continuation of the prompt. | BART uses an encoder-decoder architecture trained for sequence generation. |\n",
        "| Fill-Mask  | BERT     | Success | Correctly predicted words like \"create\" or \"generate\". | BERT is trained using Masked Language Modeling (MLM). |\n",
        "| Fill-Mask  | RoBERTa  | Success | Accurate and confident predictions for the masked word. | RoBERTa improves MLM training with more data and no NSP objective. |\n",
        "| Fill-Mask  | BART     | Partial Success | Filled the mask but with less precision. | BART supports masking but is optimized for seq2seq tasks rather than MLM. |\n",
        "| QA         | BERT     | Partial Failure | Returned incomplete or awkward answer. | Base BERT is not fine-tuned on QA datasets like SQuAD. |\n",
        "| QA         | RoBERTa  | Partial Failure | Slightly better than BERT but still inconsistent. | Encoder-only model without QA fine-tuning. |\n",
        "| QA         | BART     | Success | Produced the most coherent answer. | Encoder-decoder design helps in span generation tasks. |\n"
      ],
      "metadata": {
        "id": "6E2PRg3E0kbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT — Encoder-Only\n",
        "\n",
        "Architecture\n",
        "\n",
        "- Uses only the Encoder stack of the Transformer,\n",
        "\n",
        "- Bidirectional self-attention,\n",
        "  \n",
        "- Trained using:\n",
        "\n",
        "    Masked Language Modeling (MLM)\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "What the Encoder is good at:\n",
        "\n",
        "- Understanding entire context at once,\n",
        "\n",
        "- Learning semantic relationships,\n",
        "\n",
        "- Extracting representations for:\n",
        "\n",
        "1. Classification\n",
        "\n",
        "2. Named Entity Recognition\n",
        "\n",
        "3. Mask prediction\n",
        "\n",
        "4. Question understanding\n",
        "\n",
        "--------------------------------------------------\n",
        "Why BERT fails at Text Generation:\n",
        "\n",
        "- BERT does NOT predict the next token\n",
        "\n",
        "- It sees the entire sentence simultaneously\n",
        "\n",
        "- There is no left-to-right decoding loop\n",
        "\n",
        "- Cannot autoregressively generate new words\n",
        "\n",
        "Result:\n",
        "\n",
        "Failure... nonsense or runtime errors\n",
        "\n",
        "Why:\n",
        "\n",
        "An encoder cannot generate sequences because it lacks a decoding mechanism and causal masking\n",
        "\n",
        "-----------------------------------------------------\n",
        "Why BERT succeeds at Fill-Mask\n",
        "\n",
        "- MLM is exactly how BERT was trained\n",
        "\n",
        "- Predicts missing tokens using both left and right context\n",
        "\n",
        "Result in Experiment 2:\n",
        "\n",
        "Success — accurate predictions like create, generate\n",
        "\n",
        "Why:\n",
        "\n",
        "BERT's bidirectional encoder is optimized for masked token prediction.\n",
        "\n",
        "-------------------------------------------------------\n",
        "Why BERT is Weak at QA\n",
        "\n",
        "- Base BERT is not fine-tuned for span extraction\n",
        "\n",
        "- It understands the question, but struggles to select answer boundaries\n",
        "\n",
        "Result in Experiment 3:\n",
        "\n",
        "Partial / unreliable output\n",
        "\n",
        "Why:\n",
        "\n",
        "Encoder understands context but lacks task-specific decoding logic."
      ],
      "metadata": {
        "id": "UIoobApR2k89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "vV7oXMxw3_M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "EX3X_f9i5r7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RoBERTa — Optimized Encoder-Only Architecture\n",
        "\n",
        "Architecture\n",
        "\n",
        "- Same encoder-only structure as BERT\n",
        "\n",
        "- Improvements:\n",
        "\n",
        "1. More data\n",
        "\n",
        "2. Larger batches\n",
        "\n",
        "3. No NSP objective\n",
        "\n",
        "4. Dynamic masking\n",
        "\n",
        "-----------------------------------------------------\n",
        "Behavior Summary\n",
        "\n",
        "RoBERTa behaves almost identically to BERT, but slightly better due to training improvements.\n",
        "\n",
        "Why RoBERTa fails at generation\n",
        "\n",
        "- Still encoder-only\n",
        "\n",
        "- No autoregressive decoder\n",
        "\n",
        "- Cannot predict token sequences step-by-step\n",
        "\n",
        "Result:\n",
        "\n",
        "Failure in text generation\n",
        "\n",
        "Why:\n",
        "\n",
        "Architecture lacks a decoder and causal attention mechanism.\n",
        "\n",
        "--------------------------------------------------------\n",
        "Why RoBERTa excels at Fill-Mask\n",
        "\n",
        "- MLM is its primary training objective\n",
        "\n",
        "- Even better contextual understanding than BERT\n",
        "\n",
        "Result:\n",
        "\n",
        "Strong success in masked word prediction\n",
        "\n",
        "Why RoBERTa is inconsistent at QA\n",
        "\n",
        "- Better language understanding than BERT\n",
        "\n",
        "- Still not trained for answer span generation\n",
        "\n",
        "Result:\n",
        "\n",
        "Partial success\n",
        "\n",
        "Why:\n",
        "\n",
        "Encoder-only model without QA fine-tuning."
      ],
      "metadata": {
        "id": "jfg-ks7w3gR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "poGisldg5la0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "lw29x_Nj6bnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART — Encoder-Decoder (Seq2Seq) Architecture\n",
        "Architecture\n",
        "\n",
        "- Full Transformer Encoder + Transformer Decoder\n",
        "\n",
        "- Decoder uses:\n",
        "\n",
        "1. Causal (left-to-right) masking\n",
        "\n",
        "2. Cross-attention to encoder output\n",
        "\n",
        "3. Trained as a denoising autoencoder\n",
        "\n",
        "-----------------------------------------------\n",
        "Why BART succeeds at Text Generation\n",
        "\n",
        "- Decoder predicts tokens one at a time\n",
        "\n",
        "- Uses previous output tokens as input\n",
        "\n",
        "- Exactly matches generation task requirements\n",
        "\n",
        "Result in Experiment 1:\n",
        "\n",
        "Success — fluent text continuation\n",
        "\n",
        "Why:\n",
        "\n",
        "Encoder-decoder models are designed for sequence generation tasks.\n",
        "\n",
        "-----------------------------------------------------\n",
        "Why BART is weaker at Fill-Mask\n",
        "\n",
        "- Masking is used only as pretraining noise\n",
        "\n",
        "- Not optimized for precise single-token prediction\n",
        "\n",
        "Result in Experiment 2:\n",
        "\n",
        "Partial success\n",
        "\n",
        "Why:\n",
        "\n",
        "BART prioritizes reconstructing full sequences, not isolated mask prediction.\n",
        "\n",
        "-------------------------------------------------------\n",
        "Why BART performs best in QA\n",
        "\n",
        "- Encoder understands context\n",
        "\n",
        "- Decoder generates answer spans\n",
        "\n",
        "- Better suited for extractive/generative QA\n",
        "\n",
        "Result in Experiment 3:\n",
        "\n",
        "Most coherent answers\n",
        "\n",
        "Why:\n",
        "\n",
        "Encoder-decoder architecture naturally supports answer generation."
      ],
      "metadata": {
        "id": "Qhe-LWF954n5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBcQ-WEV6Y1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}